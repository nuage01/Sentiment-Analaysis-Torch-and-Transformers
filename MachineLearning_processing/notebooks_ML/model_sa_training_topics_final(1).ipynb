{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RrTpzBEuBUh"
   },
   "source": [
    "### <font color='green'>Verify if the GPU device is available </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3ErJS2dMWOx"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ay-tfXZu0qx8"
   },
   "source": [
    "### <font color='green'>Verify if the GPU device is available </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61krix3qXEra"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPaN1dhU03nL"
   },
   "source": [
    "### <font color='green'>Installing required libraries </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UVUPuDyO3QW"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVi9ASAqMcVU"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install -I keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWZL8zvdRld5"
   },
   "outputs": [],
   "source": [
    "!pip install -I tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVR7s56lNDb1"
   },
   "outputs": [],
   "source": [
    "#!pip install keras==2.2.4\n",
    "#!pip install tensorflow-gpu==1.13.1\n",
    "!pip uninstall -y tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIFcqg812PNa"
   },
   "source": [
    "### <font color='green'>Importing data from GCP</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxLWPReLNcVG"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
    "project_id = 'dataimpact-rd'\n",
    "!gcloud config set project {project_id}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bi4K2MegN3xg"
   },
   "outputs": [],
   "source": [
    "# Download the file from a given Google Cloud Storage bucket.\n",
    "!gsutil cp gs://di_data_sas/EN/US/Amazon/Data/periode_11/amazon_ml_opinions_topics.csv /tmp/amazon_ml_opinions_topics.csv\n",
    "  \n",
    "# Print the result to make sure the transfer worked.\n",
    "!head -n 5 /tmp/amazon_ml_opinions_topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCH9hMDYN83x"
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://di_data_sas/EN/US/Walmart/Data/2020_periode_1/walmart_ml_opinions.csv /tmp/walmart_ml_opinions_topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ol93aHqyN_Sq"
   },
   "outputs": [],
   "source": [
    "!gsutil cp  gs://di_data_sas/EN/US/Target/Data/2020_periode_1/target_ml_opinions.csv /tmp/target_ml_opinions_topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08qhPDi1OAyV"
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://di_data_sas/EN/UK/Asda/Data/2020_periode_1/asda_ml_opinions.csv /tmp/asda_ml_opinions_topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KcGvVPSOCM6"
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://di_data_sas/EN/UK/Morrisons/Data/2020_periode_1/morrisons_ml_opinions.csv /tmp/morrisons_ml_opinions_topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjSFWfM5ODpa"
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://di_data_sas/EN/UK/Ocado/Data/2020_periode_1/ocado_ml_opinions.csv  /tmp/ocado_ml_opinions_topics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_Ye4s_sTQGK"
   },
   "outputs": [],
   "source": [
    "# quick look at the data\n",
    "gl = pnd.read_csv('/tmp/asda_ml_opinions_topics.csv', nrows=2)\n",
    "print(gl.text.iloc[:1])\n",
    "print(gl.text_clean.iloc[:1])\n",
    "a = gl.review.iloc[:1]\n",
    "gl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dIDZgYJTaJL"
   },
   "outputs": [],
   "source": [
    "# function to check memory usage in megabytes\n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pnd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n",
    "print(mem_usage(dow))\n",
    "print(mem_usage(dow_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgmfLrUr6gxB"
   },
   "outputs": [],
   "source": [
    "# importing data from google drive \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eC7wACkkeWz1"
   },
   "outputs": [],
   "source": [
    "# checking the data usage of the sample data\n",
    "gl_int = gl.select_dtypes(include=['int'])\n",
    "converted_int = gl_int.apply(pnd.to_numeric,downcast='unsigned')\n",
    "print(mem_usage(gl_int))\n",
    "print(mem_usage(converted_int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4xROAAj3G4M"
   },
   "source": [
    "### <font color='green'>Setting the data types to optimize memory usage</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJRtgdcUgYrs"
   },
   "outputs": [],
   "source": [
    "edited_types = {\n",
    "'asin':         'object',\n",
    "'average'  :        'float16',\n",
    "'review_body'     :  'object',\n",
    "'review_date'      : 'object',\n",
    "'review_likes'     : 'object',\n",
    "'review_rating'    :'float16',\n",
    "'review_title'     : 'object',\n",
    "'five_star'        :'float16',\n",
    "'four_star'        :'float16',\n",
    "'one_star'        :'float16',\n",
    "'pp_date'          : 'object',\n",
    "'three_star'       :'float16',\n",
    "'two_star'         :'float16',\n",
    "'refpe'            : 'object',\n",
    "'text_clean'        :'object',\n",
    "'title_clean'       :'object',\n",
    "'ml_score'         :'float16',\n",
    "'text'             : 'object',\n",
    "'ml_topic'         : 'category',\n",
    "'opinion'          : 'object',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwMW0XVw3Vto"
   },
   "source": [
    "### <font color='green'>Function with garbage collector to free memory after deleting elements</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9_Twsb1ml_S"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "# df1 is not needed anymore\n",
    "def free_memo(element):\n",
    "  try:\n",
    "    del(element)\n",
    "    gc.collect()\n",
    "    print(f'element:  deleted')\n",
    "  except NameError:\n",
    "    print(f'element not in memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60Y_X_VKqJGx"
   },
   "outputs": [],
   "source": [
    "gc.get_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNwFUNtQNyIg"
   },
   "outputs": [],
   "source": [
    "  try:\n",
    "    del(y_train)\n",
    "    gc.collect()\n",
    "    print(f'element:  deleted')\n",
    "  except NameError:\n",
    "    print(f'element not in memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pe5u3_qdWU3C"
   },
   "outputs": [],
   "source": [
    "free_memo(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOo10HvRqeTh"
   },
   "outputs": [],
   "source": [
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsl8qNTymmly"
   },
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve6L1hKz3_Bc"
   },
   "source": [
    "### <font color='green'>loading data into pandas dataframe</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6bXg5rEOFlZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pnd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DehIyV7xOIQ_"
   },
   "outputs": [],
   "source": [
    "list_retailers = ['Amazon' , 'Asda', 'Morrisons', 'Ocado', 'Target', 'Walmart']\n",
    "retailers = {}\n",
    "for retailer in list_retailers:\n",
    "\n",
    "    retailers[retailer] = pnd.read_csv('/tmp/' + retailer.lower() + \n",
    "                                      '_ml_opinions_topics.csv', dtype=edited_types, nrows=20000)\n",
    "    #retailers[retailer] = retailers[retailer].dropna(subset=['review_body'])\n",
    "    \n",
    "#to_concat = [retailers[retailer][['text_clean', 'title_clean']] for retailer in list_retailers]\n",
    "to_concat = [retailers[retailer]['review_body'] for retailer in list_retailers]\n",
    "data = pnd.concat(to_concat, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "441EjQdj4Dtt"
   },
   "source": [
    "### <font color='green'>Generating Vocab using Sentencpiece</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBCaVXVtpdtZ"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "from tqdm import tqdm\n",
    "with open('EN_all_text.txt', 'w', encoding='utf-8') as f:\n",
    "    for x in tqdm(data.values):\n",
    "        f.write(x + '\\n')\n",
    "#spm.SentencePieceTrainer.Train(' --input_sentence_size=10000 --input=EN_all_text.txt --model_prefix=EN_vocab --vocab_size=7500  --split_by_whitespace=false')\n",
    "spm.SentencePieceTrainer.Train(' --input_sentence_size=20000 --input=EN_all_text.txt --model_prefix=EN_vocab --vocab_size=2500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kq9w1TC74J2u"
   },
   "source": [
    "### <font color='green'> initiating libraries and env for our model</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saryGGwcOMUI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../../../Git/phylogenetics/Sentiment_Analysis/Tools/')\n",
    "\n",
    "import os\n",
    "import pandas as pd \n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import pickle, multiprocessing, re\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import callbacks\n",
    "#from tf.keras.backend.tensorflow_backend import set_session\n",
    "from keras.optimizers import Adam\n",
    "#from machine_learning import *\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "config =  tf.compat.v1.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "#set_session( tf.compat.v1.Session(config=config))\n",
    "tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILbwqzEq5yW8"
   },
   "source": [
    "### <font color='green'> calculate topics appartenance for each review</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhOvdH1Z4ZkG"
   },
   "outputs": [],
   "source": [
    "free_memo(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qECT1N84cQK"
   },
   "outputs": [],
   "source": [
    "list_retailers = ['Amazon' , 'Asda', 'Morrisons', 'Ocado', 'Target', 'Walmart']\n",
    "retailers = {}\n",
    "for retailer in list_retailers:\n",
    "\n",
    "    retailers[retailer] = pnd.read_csv('/tmp/' + retailer.lower() + \n",
    "                                      '_ml_opinions_topics.csv', dtype=edited_types, nrows=20000)\n",
    "    #retailers[retailer] = retailers[retailer].dropna(subset=['review_body'])\n",
    "    \n",
    "to_concat = [retailers[retailer][['text_clean', 'title_clean']] for retailer in list_retailers]\n",
    "data = pnd.concat(to_concat, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5odQHg-7aEf"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0_-eW3JOR7s"
   },
   "outputs": [],
   "source": [
    "data['text'] = data['text_clean'] + ' ' + data['title_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Da-ztPy5QTuj"
   },
   "outputs": [],
   "source": [
    "data['text'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CejQOq9QVR0"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "topics =  pickle.load(open('topics.p', 'rb'), encoding='latin1')\n",
    "targets =  pickle.load(open('targets.p', 'rb'), encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QckdMhMQeD_"
   },
   "outputs": [],
   "source": [
    "all_reviews = data.text.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02PcOb725gIx"
   },
   "outputs": [],
   "source": [
    "all_reviews[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jDPYMXrQfyi"
   },
   "outputs": [],
   "source": [
    "K.set_epsilon(1e-5)\n",
    "\n",
    "def transform_value(value):\n",
    "    if value >= 0.02:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def get_topics(commentBody):\n",
    "    result = {}\n",
    "    for topic, words in topics.items():\n",
    "        total = np.sum([len(re.findall(word, commentBody)) for word in words])\n",
    "        result[topic] = total/len(commentBody.split())\n",
    "    result = {k:transform_value(v) for k,v in result.items()}\n",
    "    return (commentBody, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw_GFjDCQh97"
   },
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "#sp.Load('/content/drive/MyDrive/EN_vocab.model')\n",
    "sp.Load('EN_vocab.model')\n",
    "input_length, vocab_size  = 256, 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6whKe0LQjhB"
   },
   "outputs": [],
   "source": [
    "os.cpu_count()\n",
    "cores = cpu_count()\n",
    "p = Pool(cores)\n",
    "try:\n",
    "    res = list(tqdm(p.imap(get_topics, all_reviews), total=len(all_reviews)))\n",
    "finally:\n",
    "    p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zC3510A6MO7"
   },
   "outputs": [],
   "source": [
    "res[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czOP82pZQlQT"
   },
   "outputs": [],
   "source": [
    "review_to_topics = {k:v for k,v in res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcCXeAPU74tv"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for k, v in review_to_topics.items():\n",
    "  print(k,v)\n",
    "  i += 1\n",
    "  if i >3:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxoYA6_uQtHA"
   },
   "outputs": [],
   "source": [
    "\"\"\"review_to_topic to df\"\"\"\n",
    "new_df = pd.DataFrame(review_to_topics).T.reset_index()\n",
    "new_df = new_df.rename(index=str, columns=({'index': 'review'}))\n",
    "new_df['rep'] = list(new_df[['allergens','competition','delivery','packaging','price','taste']].values)\n",
    "review_to_topics = dict(new_df[['review', 'rep']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfILhM9sQvnY"
   },
   "outputs": [],
   "source": [
    "data['topic'] = data['text'].apply(lambda x: review_to_topics[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgbhbTk38T2H"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPokMJmc88Dx"
   },
   "source": [
    "### <font color='green'> Import machine learning module</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7neWV_4Q26z"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import gensim, nltk, re\n",
    "\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Bidirectional, Conv1D, CuDNNLSTM, Dense, Dropout, Embedding\n",
    "from tensorflow.python.keras.layers import normalization, Input, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def create_tokenizer(line):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(line)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    padded = pad_sequences(encoded, maxlen = max_length, padding = 'post')\n",
    "    \n",
    "    return padded\n",
    "\n",
    "def encode_docs_new_vocab(sp, max_length, docs):\n",
    "    \n",
    "    encoded =  [sp.EncodeAsIds(doc) for doc in docs]\n",
    "    padded = pad_sequences(encoded, maxlen = max_length, padding = 'post')\n",
    "    \n",
    "    return padded\n",
    "\n",
    "def f1(y_true, y_pred):    \n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "\n",
    "def generate_data(df, mean_length, ratio, token=None, sp=None):\n",
    "    \n",
    "    # split dataframe into singles dataframes for each rating score\n",
    "    data_1 =  df.loc[lambda df: df['review_rating'] == 1]\n",
    "    data_2 =  df.loc[lambda df: df['review_rating'] == 2]\n",
    "    data_3 =  df.loc[lambda df: df['review_rating'] == 3]\n",
    "    data_4 =  df.loc[lambda df: df['review_rating'] == 4]\n",
    "    data_5 =  df.loc[lambda df: df['review_rating'] == 5]\n",
    "    \n",
    "    # spliting each score dataframe into two dataframes set by a ratio\n",
    "    data_val_1 = data_1[:int(ratio*len(data_1))]\n",
    "    data_train_1 =  data_1[int(ratio*len(data_1)):]\n",
    "\n",
    "    data_val_2 = data_2[:int(ratio*len(data_2))]\n",
    "    data_train_2 =  data_2[int(ratio*len(data_2)):]\n",
    "\n",
    "    data_val_3 = data_3[:int(ratio*len(data_3))]\n",
    "    data_train_3 =  data_3[int(ratio*len(data_3)):]\n",
    "\n",
    "    data_val_4 = data_4[:int(ratio*len(data_4))]\n",
    "    data_train_4 =  data_4[int(ratio*len(data_4)):]\n",
    "\n",
    "    data_val_5 = data_5[:int(ratio*len(data_5))]\n",
    "    data_train_5 =  data_5[int(ratio*len(data_5)):]\n",
    "    \n",
    "    # concat dfs split by ratio\n",
    "    train_x = pd.concat([data_train_1, data_train_2,data_train_3,  data_train_4, data_train_5])\n",
    "    val_x = pd.concat([data_val_1, data_val_2,data_train_3, data_val_4, data_val_5])\n",
    "    \n",
    "    # setting positifs 1 for rating >3\n",
    "    train_x['score'] = train_x['review_rating'].apply(lambda x: 1 if x > 3 else 0)\n",
    "    val_x['score'] = val_x['review_rating'].apply(lambda x: 1 if x > 3 else 0)\n",
    "    \n",
    "    train_y = train_x['score'].values\n",
    "    val_y = val_x['score'].values\n",
    "    \n",
    "    #applying categorical from keras\n",
    "    y_train =  to_categorical(train_y)\n",
    "    y_val = to_categorical(val_y)\n",
    "    \n",
    "    # choosing tokenization by word or bpe\n",
    "    if sp == None:\n",
    "        X_train = encode_docs(token, mean_length, train_x['review_body'])\n",
    "        X_val = encode_docs(token, mean_length, val_x['review_body'])\n",
    "    else:\n",
    "        X_train = encode_docs_new_vocab(sp, mean_length, train_x['review_body'])\n",
    "        X_val = encode_docs_new_vocab(sp, mean_length, val_x['review_body'])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def ml_model_score(vocab_size, input_length, dimension):\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size, dimension, input_length=input_length)\n",
    "    sequence_input = Input(shape=(input_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(embedded_sequences)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(64,  activation = 'relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    output_tensor = Dense(2, activation = 'softmax')(x)\n",
    "    \n",
    "    return Model(sequence_input, output_tensor)\n",
    "\n",
    "def ml_model_topics(vocab_size, input_length, dimension):\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size, dimension, input_length=input_length)\n",
    "    sequence_input = Input(shape=(input_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(embedded_sequences)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(64,  activation = 'relu')(x)\n",
    "\n",
    "    output_tensor = Dense(6, activation = 'sigmoid')(x)\n",
    "    \n",
    "    return Model(sequence_input, output_tensor)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_pos = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    _precision = true_pos / (predicted_pos + K.epsilon())\n",
    "    return _precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_pos = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    _recall = true_pos / (possible_pos + K.epsilon())\n",
    "    return _recall\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmC5ZRwkQwjV"
   },
   "outputs": [],
   "source": [
    "\"\"\" split data for training\"\"\"\n",
    "val_x = data[:int(0.2*len(data))]\n",
    "train_x = data[int(0.2*len(data)):]\n",
    "\n",
    "y_train = train_x['topic'].values\n",
    "y_val = val_x['topic'].values\n",
    "\n",
    "y_train = np.asarray([list(v) for v in y_train])\n",
    "y_val = np.asarray([list(v) for v in y_val])\n",
    "\n",
    "X_train = encode_docs_new_vocab(sp, input_length, train_x['text'])\n",
    "X_val = encode_docs_new_vocab(sp, input_length, val_x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHJq3d2EC7cL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT9vOwgw9QnI"
   },
   "source": [
    "### <font color='green'> training on the hole data loaded in memory</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJu8sc7EQ9o8"
   },
   "outputs": [],
   "source": [
    "model = ml_model_topics(vocab_size, input_length, 100)\n",
    "\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint('EN_weights_model_topics.h5',\n",
    "                                       monitor='val_acc', save_best_only=True, \n",
    "                                           save_weights_only=True, verbose=0)\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-3), loss='binary_crossentropy', metrics=['accuracy', f1])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_val, y_val), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Utikg0__SGEu"
   },
   "outputs": [],
   "source": [
    "\"\"\" saving the model weights\"\"\"\n",
    "model.save('EN_weights_model_topics.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5eWZ0609isN"
   },
   "source": [
    "### <font color='green'> méthod 2: training using generators, to load batch of data at a time</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHlFqSF3XwgT"
   },
   "outputs": [],
   "source": [
    "\n",
    "# saving training and validation sets as numpy array\n",
    "np.save('X_train_file.npy', X_train)\n",
    "np.save('X_val_file.npy', X_val)\n",
    "np.save('y_val_file.npy', y_val)\n",
    "np.save('y_train_file.npy', y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35e2OhxMp_0G"
   },
   "outputs": [],
   "source": [
    "\"\"\" loading the training and validation sets after restarting env (free memory)\"\"\"\n",
    "entrain_x = np.load('/content/drive/MyDrive/X_train_file.npy')\n",
    "val_x  = np.load('/content/drive/MyDrive/X_val_file.npy')\n",
    "entrain_y = np.load('/content/drive/MyDrive/y_train_file.npy')\n",
    "val_y = np.load('/content/drive/MyDrive/y_val_file.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q13XtJozu9Ro"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class Dataset_Gen:\n",
    "    \"\"\"Dataset Generator\"\"\"\n",
    "    \n",
    "    def __init__(self, train_x, valid_x, train_y, valid_y, batch):\n",
    "\n",
    "        self.entrain_x = np.load(train_x)\n",
    "        self.entrain_y =np.load(train_y)\n",
    "        \n",
    "        self.val_x  = np.load(valid_x)\n",
    "        self.val_y =np.load(valid_y)\n",
    "\n",
    "        #Read necessary files from disk\n",
    "\n",
    "        self.batch = batch \n",
    "        \n",
    "        self.data_sample_n = 0 \n",
    "        self.v_sample_n = 0\n",
    "\n",
    "        self.train_samples = int(len(self.entrain_x)/batch)\n",
    "        self.valid_samples = int(len(self.val_x)/batch)\n",
    "\n",
    "\n",
    "    def get_train(self):\n",
    "        while True:\n",
    "            if(self.data_sample_n == self.train_samples-1 ):\n",
    "                self.data_sample_n = 0\n",
    "            \n",
    "            X = self.entrain_x[ self.data_sample_n*self.batch : self.data_sample_n*self.batch + self.batch]\n",
    "            y = self.entrain_y [ self.data_sample_n*self.batch : self.data_sample_n*self.batch + self.batch]\n",
    "\n",
    "            self.data_sample_n += 1\n",
    "\n",
    "            yield (X,y)\n",
    "\n",
    "    \n",
    "    def get_valid(self):\n",
    "        while True:\n",
    "            if(self.v_sample_n == self.valid_samples-1 ):\n",
    "                self.v_sample_n = 0\n",
    "            \n",
    "            X = self.val_x[ self.v_sample_n*self.batch : self.v_sample_n*self.batch + self.batch]\n",
    "            y = self.val_y[ self.v_sample_n*self.batch : self.v_sample_n*self.batch + self.batch]\n",
    "            self.v_sample_n += 1\n",
    "\n",
    "            yield (X,y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U52r86UDsXfK"
   },
   "outputs": [],
   "source": [
    "#dg = Dataset_Gen('/content/drive/MyDrive/X_train_file.npy', '/content/drive/MyDrive/X_val_file.npy', '/content/drive/MyDrive/y_train_file.npy', '/content/drive/MyDrive/y_val_file.npy', 128)\n",
    "dg = Dataset_Gen('X_train_file.npy', 'X_val_file.npy', 'y_train_file.npy', 'y_val_file.npy', 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqAfyzRTzCSA"
   },
   "outputs": [],
   "source": [
    "model = ml_model_topics(vocab_size, input_length, 100)\n",
    "\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint('EN_weights_model_topics.h5',\n",
    "                                       monitor='val_accuracy', save_best_only=True, \n",
    "                                           save_weights_only=True, verbose=0)\n",
    "# lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * (0.95 ** epoch))\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-3), loss='binary_crossentropy', metrics=['accuracy', f1])\n",
    "\n",
    "model.fit_generator(dg.get_train(),dg.train_samples, epochs=10, initial_epoch=0, validation_data= dg.get_valid(), validation_steps = dg.valid_samples, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfmnlP_6acr_"
   },
   "source": [
    "### <font color='green'>Model plotting</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3xjcE30bJ_D"
   },
   "outputs": [],
   "source": [
    "dot_img_file = 'model_1.png'\n",
    "tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJGEg3JWTpi1"
   },
   "outputs": [],
   "source": [
    "\"\"\"keras model to dot will Convert a Keras model to dot format.\"\"\"\n",
    "from keras.utils.vis_utils import plot_model,model_to_dot\n",
    "from IPython.display import SVGA\n",
    "SVG(model_to_dot(model ,show_shapes=True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CL-q_6AK-gQp"
   },
   "outputs": [],
   "source": [
    "\"\"\" saving the model weights\"\"\"\n",
    "model.save('EN_weights_model_topics.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LixW4voKrFEV"
   },
   "outputs": [],
   "source": [
    "\"\"\"another method for the generator\"\"\"\n",
    "import random\n",
    "def generator(features, labels, batch_size): # Create empty arrays to contain batch of features and labels# \n",
    "  batch_features = np.zeros((batch_size, 256))\n",
    "  batch_labels = np.zeros((batch_size, 6)) \n",
    "  while True:\n",
    "    for i in range(batch_size):\n",
    "      # choose random index in features\n",
    "      #index= random.choice(len(features),1)\n",
    "      index= i \n",
    "      batch_features[i] = features[index]\n",
    "      batch_labels[i] = labels[index]\n",
    "    yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vfgJfQuaPRS"
   },
   "source": [
    "### <font color='green'>Evaluating the model</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-SpzHaFbrR5"
   },
   "outputs": [],
   "source": [
    "\"\"\" loading the training and validation sets after restarting env (free memory)\"\"\"\n",
    "\n",
    "val_x  = np.load('X_val_file.npy')\n",
    "val_y = np.load('y_val_file.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxeB5HLraKS8"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(val_x , val_y, batch_size=128)\n",
    "print(\"model accuracy on test data: \", f'{round(results[1], 5)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpJD-isjb4Aw"
   },
   "outputs": [],
   "source": [
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(val_x[:20])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gakMZ8PDyINJ"
   },
   "outputs": [],
   "source": [
    "predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gACQbW0D1F0g"
   },
   "outputs": [],
   "source": [
    "print(dg.valid_samples)\n",
    "print(len(dg.val_x)/128)\n",
    "print(len(y_pred))\n",
    "print(y_pred[:5])\n",
    "print(type(np.asarray(y_pred[:10])))\n",
    "type(one_hot_to_numbers(dg.val_y[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zk1vZItE6QIX"
   },
   "outputs": [],
   "source": [
    "def numpy_to_list(labels):\n",
    "  r_lables = []\n",
    "  for tab in labels:\n",
    "    r_lables.append(tab.tolist())\n",
    "  return r_lables\n",
    "\n",
    "def one_hot_to_numbers(labels):\n",
    "  r_lables = []\n",
    "  for tab in labels:\n",
    "    try:\n",
    "      r_lables.append(tab.tolist().index(1))\n",
    "    except:\n",
    "      r_lables.append(0)\n",
    "  return np.array(r_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNB3CpbiybUM"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Y_pred = model.predict_generator(dg.get_valid(), dg.valid_samples)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(one_hot_to_numbers(dg.val_y[:len(y_pred)]), y_pred))\n",
    "print('Classification Report')\n",
    "print(classification_report(one_hot_to_numbers(dg.val_y[:len(y_pred)]), y_pred, target_names=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nf3XykAJcQY7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "def plot_eval(y_true, y_pred):\n",
    "\n",
    "  y_true_names = y_true\n",
    "  y_pred_names = y_pred\n",
    "  print( classification_report(y_true_names, y_pred_names) )\n",
    "  cm = confusion_matrix(y_true_names, y_pred_names) \n",
    "  labels = targets\n",
    "  df_cm = pnd.DataFrame(cm, index=labels, columns=labels)\n",
    "  # config plot sizes\n",
    "  sn.set(font_scale=1.2)\n",
    "  sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 18}, cmap='coolwarm', linewidth=0.5, fmt=\"\")\n",
    "  plt.title('confusion matrix')\n",
    "  plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4x0_MfzdBlP4"
   },
   "outputs": [],
   "source": [
    "plot_eval(one_hot_to_numbers(dg.val_y[:len(y_pred)]), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9aIKthGxxiX"
   },
   "outputs": [],
   "source": [
    "probs = np.exp(predictions[:,1])\n",
    "probs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_sa_training_topics_final.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

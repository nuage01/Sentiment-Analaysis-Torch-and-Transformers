{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dateparser in /home/lyes/Crawling/env3/lib/python3.6/site-packages (1.0.0)\n",
      "Requirement already satisfied: python-dateutil in /home/lyes/Crawling/env3/lib/python3.6/site-packages (from dateparser) (2.8.1)\n",
      "Requirement already satisfied: regex!=2019.02.19 in /home/lyes/Crawling/env3/lib/python3.6/site-packages (from dateparser) (2020.11.13)\n",
      "Requirement already satisfied: tzlocal in /home/lyes/Crawling/env3/lib/python3.6/site-packages (from dateparser) (2.1)\n",
      "Requirement already satisfied: pytz in /home/lyes/Crawling/env3/lib/python3.6/site-packages (from dateparser) (2020.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/lyes/Crawling/env3/lib/python3.6/site-packages (from python-dateutil->dateparser) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/home/lyes/Crawling/env3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dateparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: dataclasses\n",
      "Successfully installed dataclasses-0.8\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/home/lyes/Crawling/env3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global stopwords\n",
    "import importlib\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "global stopwords\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from pandarallel import pandarallel\n",
    "from string import punctuation\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords = [word for word in stopwords if word != 'not']\n",
    "\n",
    "process_functions = importlib.import_module('process_function_definitions_EN')\n",
    "\n",
    "\n",
    "TO_REMOVE = [x for x in punctuation]\n",
    "\n",
    "def process_data(df, retailer, country):\n",
    "    \n",
    "    df.columns = map(str.lower, df.columns)\n",
    "\n",
    "    if('title' not in df):\n",
    "        df['title'] = ''\n",
    "        \n",
    "    if('crawl_date' not in df):\n",
    "        df = df.rename(index=str, columns={\"content\": \"review_body\", \"date\": \"review_date\", \"product_id\": \"asin\",\n",
    "        \"product_ean\": \"ean\", \"rating\": \"review_rating\", \"title\": \"review_title\", 'crawling_date': 'pp_date'}) \n",
    "    else:\n",
    "        df = df.rename(index=str, columns={\"content\": \"review_body\", \"date\": \"review_date\", \"product_id\": \"asin\",\n",
    "        \"product_ean\": \"ean\", \"rating\": \"review_rating\", \"title\": \"review_title\", 'crawl_date': 'pp_date'}) \n",
    "\n",
    "    df = df[['review_body', 'review_title', 'asin', 'pp_date', 'review_rating', 'review_date']]\n",
    "\n",
    "    df = df.drop_duplicates(keep='first')\n",
    "    df = df.dropna(subset=['review_body'])\n",
    "    df = df.dropna(subset=['asin'])\n",
    "    df = df.dropna(subset=['pp_date'])\n",
    "    df = df.dropna(subset=['review_rating'])\n",
    "    df = df.dropna(subset=['review_date'])\n",
    "\n",
    "    df['review_title'] = df['review_title'].str.lower()\n",
    "    df['review_body'] = df['review_body'].str.lower()\n",
    "\n",
    "    df['review_rating'] = df['review_rating'].apply(lambda x: int(x[0]) if isinstance(x, str) else x)\n",
    "    df = df[df['review_rating'] != 0]\n",
    "\n",
    "    \n",
    "    df['review_title'].fillna('',  inplace=True)\n",
    "    df['review_title'] = df['review_title'].apply(lambda x: \n",
    "                            x.replace('none',''))\n",
    "\n",
    "    df['review_body'] = df['review_body'].apply(lambda x: \n",
    "                            x.replace(' [this review was collected as part of a promotion.]',''))\n",
    "\n",
    "    df['text_clean'] = df['review_body']\n",
    "    df['title_clean'] = df['review_title']\n",
    "\n",
    "    df['review_date'] = df['review_date'].apply(lambda x: process_functions.parse_date(x))\n",
    "    \n",
    "    for char in tqdm(TO_REMOVE):\n",
    "        clean = process_functions.Compose([process_functions.Replace_Char_With_White(char)])\n",
    "        df = clean.apply_on_df(df, 'text_clean')\n",
    "        df = clean.apply_on_df(df, 'title_clean')\n",
    "    \n",
    "    clean1 = process_functions.Compose([process_functions.Regex_Sub(), \n",
    "                                       process_functions.Expand(), \n",
    "                                       process_functions.DeEmojify(), \n",
    "                                       process_functions.Remove_Num()\n",
    "                                      ])\n",
    "    \n",
    "    clean2 = process_functions.Compose([process_functions.Replace_Stopword(stopwords),\n",
    "                                       process_functions.Remove_One_Character()\n",
    "                                      ])\n",
    "    \n",
    "    df = clean1.apply_on_df(df, 'text_clean')\n",
    "    df = clean1.apply_on_df(df, 'title_clean')\n",
    "\n",
    "    df = clean2.apply_on_df(df, 'text_clean')\n",
    "    df = clean2.apply_on_df(df, 'title_clean')\n",
    "    df.to_csv(retailer.lower() + '_cleaned.csv', index=False)\n",
    "    logging.info('Data cleaned !')\n",
    "    print('Data cleaned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('takealot_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Run the cleaning</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned\n"
     ]
    }
   ],
   "source": [
    "process_data(df, 'takealot', 'UK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Parssing opinions</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, importlib\n",
    "from tqdm import tqdm\n",
    "import ast, pickle, spacy\n",
    "from nltk.corpus import stopwords\n",
    "from multiprocessing import cpu_count, Pool\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "def get_info(retailer, country):\n",
    "    \n",
    "    lang, nlp_model, stop_words = 'EN', 'en_core_web_lg', set(stopwords.words('english'))\n",
    "    if country in ('FR', 'BE'):\n",
    "        lang, nlp_model, stop_words = 'FR', 'fr_core_news_md', set(stopwords.words('french'))\n",
    "        \n",
    "    global list_pos\n",
    "    list_pos = ['ADJ NOUN', 'ADJ NOUN NOUN', 'ADV ADJ', 'ADV ADJ NOUN', 'ADV VERB', 'ADV VERB ADJ', 'VERB NOUN',\n",
    "                'VERB ADV', 'VERB PRON PART VERB ADJ', 'ADV ADV PRON VERB', 'ADV VERB PRON', 'NOUN VERB ADJ', \n",
    "                'ADJ NOUN CCONJ NOUN', 'NOUN ADP PRON', 'VERB ADV ADJ', 'PRON VERB VERB', 'PRON VERB ADJ ADV',\n",
    "                'ADV ADV ADJ', 'ADJ PUNCT ADJ NOUN NOUN', 'NOUN VERB ADV', 'ADV NOUN PRON VERB', 'ADJ CCONJ VERB ADJ', \n",
    "                'ADJ NOUN ADJ NOUN', 'ADJ ADJ NOUN']\n",
    "    \n",
    "    if country == 'FR':\n",
    "        list_pos = ['ADJ NOUN', 'NOUN ADJ', 'NOUN ADJ NOUN', 'NOUN NOUN NOUN ADJ', 'ADV NOUN', 'NOUN ADV ADJ', \n",
    "            'NOUN ADP ADJ', 'NOUN ADP NOUN', 'ADV ADV NOUN', 'ADV ADP', 'VERB ADJ', 'ADJ ADJ', \n",
    "            'ADJ NOUN ADJ', 'ADP ADJ', 'NOUN NOUN', 'NOUN NOUN ADP NOUN', 'ADV ADJ NOUN', 'NOUN ADV NOUN',\n",
    "            'NOUN ADP NOUN ADV', 'NOUN ADJ ADV ADJ', 'NOUN NOUN NOUN', 'NOUN NOUN ADP NOUN', 'ADV ADP NOUN',\n",
    "            'NOUN ADP NOUN', 'ADJ NOUN NOUN NOUN', 'ADJ NOUN NOUN ADJ', 'ADJ NOUN ADP NOUN', 'ADJ NOUN ADJ NOUN', \n",
    "            'ADV ADV NOUN ADJ', 'NOUN ADP NOUN ADJ', 'ADV ADJ NOUN', 'NOUN ADJ ADV ADJ NOUN', \n",
    "            'NOUN ADJ ADP NOUN', 'NOUN ADV ADP NOUN', 'NOUN ADP VERB', 'NOUN ADV VERB', 'NOUN VERB ADV ADJ',\n",
    "            'NOUN ADJ ADV ADJ', 'NOUN ADP NOUN ADJ', 'ADJ ADP NOUN', 'NOUN ADV NOUN', 'ADV ADP NOUN']\n",
    "        \n",
    "    return lang, nlp_model, country, retailer, stop_words, list_pos\n",
    "\n",
    "def comment_to_opinion(t):\n",
    "\n",
    "    opinion = text_to_nlp[t]\n",
    "    opinion = generate_phrase(opinion, list_pos) \n",
    "\n",
    "    return (t,opinion)\n",
    "\n",
    "def generate_phrase(text, list_pos):\n",
    "    \n",
    "    res = []\n",
    "    for sent in text.noun_chunks:\n",
    "        found = False\n",
    "        op, pos = [], []\n",
    "        for token in sent:\n",
    "            p = token.pos_\n",
    "            if p != 'PUNCT' and p != 'DET':\n",
    "                to_add = str(token.lemma_)\n",
    "                if to_add in opinions:\n",
    "                    found = True\n",
    "                op.append(to_add)\n",
    "                pos.append(p)\n",
    "        if found:\n",
    "            op = ' '.join(op)\n",
    "            pos = ' '.join(pos)\n",
    "            if pos in list_pos:\n",
    "                res.append(op)\n",
    "    return set(res)\n",
    "\n",
    "def read_opinions_lexicon(lang):\n",
    "  \n",
    "    lexico_p = pd.read_csv('positive_words.txt', encoding=\"ISO-8859-1\", sep=\"\\n\", header = None)\n",
    "    lexico_p.columns = ['opinion_words']\n",
    "    lexico_p = lexico_p.loc[:].reset_index(drop=True)\n",
    "    set_p = set(lexico_p['opinion_words'])\n",
    "    \n",
    "    lexico_n = pd.read_csv('negative_words.txt', encoding=\"ISO-8859-1\", sep=\"\\n\", header = None)\n",
    "    lexico_n.columns = ['opinion_words']\n",
    "    lexico_n = lexico_n.loc[:].reset_index(drop=True)\n",
    "    set_n = set(lexico_n['opinion_words'])\n",
    "\n",
    "    return set_n | set_p\n",
    "\n",
    "def process_opinions(retailer, country=\"UK\" ,batch_size=50):\n",
    "    cores = 6\n",
    "    lang, nlp_model, country, retailer, stop_words, list_pos = get_info(retailer, country)\n",
    "    nlp = en_core_web_sm.load()\n",
    "    \n",
    "    global opinions\n",
    "    opinions = list(read_opinions_lexicon(lang))\n",
    "    \n",
    "    df =  pd.read_csv(retailer.lower() + '_cleaned.csv')\n",
    "    #df = df.dropna(subset=['ml_score'])\n",
    "    df['review_title'] = df['review_title'].replace(np.nan, '', regex=True)\n",
    "    df['review'] = df['review_body'] + ' ' + df['review_title']\n",
    "\n",
    "    count = 0\n",
    "    to_process_text = []\n",
    "    for text in df['review'].unique():\n",
    "        to_process_text.append(text)\n",
    "        count += 1 \n",
    "\n",
    "    global text_to_nlp\n",
    "\n",
    "    new_text_to_opinions = {}\n",
    "    text_to_nlp = {}\n",
    "\n",
    "    _batch = 5000\n",
    "    if int(count/batch_size) + 1 > 100:\n",
    "        _batch = 3500\n",
    "\n",
    "    for b in range(0, int(count/batch_size) + 1):\n",
    "        _temp = to_process_text[b*batch_size:(b+1)*batch_size]\n",
    "        nlp_text = [doc for doc in tqdm(nlp.pipe(_temp, batch_size=_batch))]\n",
    "        text_to_nlp = {i:j for i,j in zip(_temp, nlp_text)}\n",
    "        args = [elements for elements in tqdm(_temp)]\n",
    "\n",
    "        p = Pool(cores)\n",
    "        try:\n",
    "            res = list(tqdm(p.imap(comment_to_opinion, args), total=len(args)))\n",
    "        finally:\n",
    "            p.close()\n",
    "        for k,v in res:\n",
    "            if v != 0:\n",
    "                new_text_to_opinions[k] = v\n",
    "\n",
    "    df['opinion'] = df['review'].apply(lambda x: list(new_text_to_opinions[x]) \n",
    "                                                                  if x in new_text_to_opinions.keys() else [''])\n",
    "\n",
    "    df.to_csv(retailer.lower() +  '_processed_opinions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Runing parsing</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "process_opinions('takealot', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

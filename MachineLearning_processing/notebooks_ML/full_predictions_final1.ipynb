{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"full_predictions_final1.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pMCxc6iEMlvX"},"source":["### <font color='green'>Predictions here</font> \n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"QZSyZLXbMYsw"},"source":["# GPU to use if using CUDA model, in our case at a small scale we are going to use LSTM instead of CuDNNLSTM\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P_xupfHmKwsM"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gV4rcCSAMtGI"},"source":["!pip install sentencepiece\n","!pip install -I tensorflow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WDvgxSvRM3Dl"},"source":["#!/usr/bin/python\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd \n","\n","import gensim, nltk, re\n","\n","from tensorflow.python.keras import regularizers\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras.models import Model\n","from tensorflow.python.keras.layers import Bidirectional, Conv1D, CuDNNLSTM, Dense, Dropout, Embedding, LSTM\n","from tensorflow.python.keras.layers import normalization, Input, MaxPooling1D, GlobalMaxPooling1D\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import confusion_matrix\n","\n","def create_tokenizer(line):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(line)\n","    \n","    return tokenizer\n","\n","def encode_docs(tokenizer, max_length, docs):\n","\n","    encoded = tokenizer.texts_to_sequences(docs)\n","    padded = pad_sequences(encoded, maxlen = max_length, padding = 'post')\n","    \n","    return padded\n","\n","def encode_docs_new_vocab(sp, max_length, docs):\n","    \n","    encoded =  [sp.EncodeAsIds(doc) for doc in docs]\n","    padded = pad_sequences(encoded, maxlen = max_length, padding = 'post')\n","    \n","    return padded\n","\n","def f1(y_true, y_pred):    \n","    p = precision(y_true, y_pred)\n","    r = recall(y_true, y_pred)\n","    return 2 * ((p * r) / (p + r + K.epsilon()))\n","\n","\n","def generate_data(df, mean_length, ratio, token=None, sp=None):\n","    \n","    # split dataframe into singles dataframes for each rating score\n","    data_1 =  df.loc[lambda df: df['review_rating'] == 1]\n","    data_2 =  df.loc[lambda df: df['review_rating'] == 2]\n","    data_3 =  df.loc[lambda df: df['review_rating'] == 3]\n","    data_4 =  df.loc[lambda df: df['review_rating'] == 4]\n","    data_5 =  df.loc[lambda df: df['review_rating'] == 5]\n","    \n","    # spliting each score dataframe into two dataframes set by a ratio\n","    data_val_1 = data_1[:int(ratio*len(data_1))]\n","    data_train_1 =  data_1[int(ratio*len(data_1)):]\n","\n","    data_val_2 = data_2[:int(ratio*len(data_2))]\n","    data_train_2 =  data_2[int(ratio*len(data_2)):]\n","\n","    data_val_3 = data_3[:int(ratio*len(data_3))]\n","    data_train_3 =  data_3[int(ratio*len(data_3)):]\n","\n","    data_val_4 = data_4[:int(ratio*len(data_4))]\n","    data_train_4 =  data_4[int(ratio*len(data_4)):]\n","\n","    data_val_5 = data_5[:int(ratio*len(data_5))]\n","    data_train_5 =  data_5[int(ratio*len(data_5)):]\n","    \n","    # concat dfs split by ratio\n","    train_x = pd.concat([data_train_1, data_train_2,data_train_3,  data_train_4, data_train_5])\n","    val_x = pd.concat([data_val_1, data_val_2,data_train_3, data_val_4, data_val_5])\n","    \n","    # setting positifs 1 for rating >3\n","    train_x['score'] = train_x['review_rating'].apply(lambda x: 1 if x > 3 else 0)\n","    val_x['score'] = val_x['review_rating'].apply(lambda x: 1 if x > 3 else 0)\n","    \n","    train_y = train_x['score'].values\n","    val_y = val_x['score'].values\n","    \n","    #applying categorical from keras\n","    y_train =  to_categorical(train_y)\n","    y_val = to_categorical(val_y)\n","    \n","    # choosing tokenization by word or bpe\n","    if sp == None:\n","        X_train = encode_docs(token, mean_length, train_x['review_body'])\n","        X_val = encode_docs(token, mean_length, val_x['review_body'])\n","    else:\n","        X_train = encode_docs_new_vocab(sp, mean_length, train_x['review_body'])\n","        X_val = encode_docs_new_vocab(sp, mean_length, val_x['review_body'])\n","    \n","    return X_train, y_train, X_val, y_val\n","\n","def ml_model_score(vocab_size, input_length, dimension):\n","    \n","    embedding_layer = Embedding(vocab_size, dimension, input_length=input_length)\n","    sequence_input = Input(shape=(input_length,), dtype='int32')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    x = Bidirectional(LSTM(64, return_sequences=False))(embedded_sequences)\n","    x = Dropout(0.4)(x)\n","    x = Dense(64,  activation = 'relu')(x)\n","    x = Dropout(0.3)(x)\n","\n","    output_tensor = Dense(2, activation = 'softmax')(x)\n","    \n","    return Model(sequence_input, output_tensor)\n","\n","def ml_model_topics(vocab_size, input_length, dimension):\n","    \n","    embedding_layer = Embedding(vocab_size, dimension, input_length=input_length)\n","    sequence_input = Input(shape=(input_length,), dtype='int32')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    x = Bidirectional(LSTM(64, return_sequences=False))(embedded_sequences)\n","    x = Dropout(0.4)(x)\n","    x = Dense(64,  activation = 'relu')(x)\n","\n","    output_tensor = Dense(6, activation = 'sigmoid')(x)\n","    \n","    return Model(sequence_input, output_tensor)\n","\n","\n","def precision(y_true, y_pred):\n","    true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_pos = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    _precision = true_pos / (predicted_pos + K.epsilon())\n","    return _precision\n","\n","def recall(y_true, y_pred):\n","    true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_pos = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    _recall = true_pos / (possible_pos + K.epsilon())\n","    return _recall\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZrxP4pR1M_Pv"},"source":["\n","import importlib\n","import pickle, os\n","import numpy as np\n","import pandas as pd \n","from tqdm import tqdm\n","import tensorflow as tf\n","from keras import callbacks\n","import sentencepiece as spm\n","from keras import backend as K\n","from keras.optimizers import Adam\n","# from keras.backend.tensorflow_backend import set_session\n","# MACHINE_LEARNING = importlib.import_module('src.3_Prediction.ML.machine_learning')\n","\n","results = None\n","PRED_Y = None\n","TRUE_y = None\n","\n","def predict_reviews(retailer, country=\"UK\"):\n","    \n","    config =  tf.compat.v1.ConfigProto() \n","    config.gpu_options.per_process_gpu_memory_fraction = 0.8\n","    tf.compat.v1.Session(config=config)\n","\n","    K.set_epsilon(1e-5)\n","    country = 'US'\n","    lang = 'EN'\n","    if country in ('FR', 'BE'):\n","        lang = 'FR'\n","\n","    df = pd.read_csv(retailer + '_processed_opinions.csv')\n","    df = df.dropna(subset=['review_body'])\n","\n","    sp = spm.SentencePieceProcessor()\n","    sp.Load('vocab.model')\n","\n","    input_length, vocab_size  = 256, 7500\n","\n","    model = ml_model_score(vocab_size, input_length, 100)\n","    model.compile(optimizer=Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy', f1])\n","    model.load_weights('score.h5')\n","\n","    X = encode_docs_new_vocab(sp, input_length, df['review_body'])\n","    Y_true = df['review_rating'].apply(lambda x: 1 if x > 3 else 0)\n","    Y = model.predict(X, batch_size=5000)\n","\n","    target_all = [1 if i > 0.5 else -1 for i in Y_true]\n","    pred_all = [1 if i[0] < 0.5 else -1 for i in Y]\n","\n","    pickle.dump(target_all, open('target_all.p', 'wb'))\n","    pickle.dump(pred_all, open('pred_all.p', 'wb'))\n","\n","\n","    _temp = pd.DataFrame(Y)\n","    _temp['ml_score'] = _temp[0].apply(lambda x: 1 if x < 0.5 else -1)\n","\n","    df['ml_score'] = _temp['ml_score']\n","    df['ml_score']= df[['ml_score','review_rating']].apply(lambda x: x['ml_score'] if x['review_rating'] > 1 else -1, axis=1)\n","\n","\n","    df['text_clean'] = df['text_clean'].replace(np.nan, '', regex=True)\n","    df['title_clean'] = df['title_clean'].replace(np.nan, '', regex=True)\n","\n","    df['text'] = df['text_clean'] + ' ' + df['title_clean']\n","    df['text'].fillna('', inplace=True)\n","\n","    model = ml_model_topics(vocab_size, input_length, 100)\n","    model.compile(optimizer=Adam(lr=1e-3), loss='binary_crossentropy', metrics=['accuracy', f1])\n","    model.load_weights('topics.h5')\n","\n","    X = encode_docs_new_vocab(sp, input_length, df['text'])\n","    Y = model.predict(X, batch_size=5000)\n","\n","    targets = pickle.load(open('targets.p', 'rb'))\n","    new_y = [[targets[index] if element > 0.95 else 0 for index, element in enumerate(elements)] for elements in Y]\n","\n","    _temp = pd.DataFrame(new_y)\n","    df['ml_topic'] = list(_temp[[0,1,2,3,4,5]].values)\n","\n","    df['ml_topic'] = df['ml_topic'].apply(lambda x: [i for i in x if i != 0])\n","\n","    df.to_csv(retailer.lower() + '_final_ml_processed.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdu3V0W6dWpA"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7xlW0z8OuVt"},"source":["predict_reviews('takealot')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hybEM0VWZgc"},"source":["import pandas as pnd\n","df = pnd.read_csv('takealot_final_ml_processed.csv')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9WKGVsG3YLzC"},"source":["df.sample(3).loc[:, [ 'review_body','ml_score','ml_topic','opinion']].style.applymap(lambda x: 'background-color: crimson; font-size:20px; border:solid')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4h1zinGOxU3"},"source":["from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","import pandas as pnd\n","\n","def plot_eval(y_true, y_pred):\n","\n","  y_true_names = y_true\n","  y_pred_names = y_pred\n","  print( classification_report(y_true_names, y_pred_names) )\n","  cm = confusion_matrix(y_true_names, y_pred_names) \n","  labels = ['NEGATIVE', 'POSTITF']\n","  df_cm = pnd.DataFrame(cm, index=labels, columns=labels)\n","  # config plot sizes\n","  sn.set(font_scale=1.2)\n","  sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 18}, cmap='coolwarm', linewidth=0.5, fmt=\"\")\n","  plt.title('confusion matrix')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bEJe_gSOaCia"},"source":["target_all  =  pickle.load(open('target_all.p', 'rb'), encoding='latin1')\n","pred_all =  pickle.load(open('pred_all.p', 'rb'), encoding='latin1')\n","plot_eval(target_all , pred_all)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Hulc7JiacRv"},"source":["df = pd.read_csv('take.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvYP7Oqkb3S2"},"source":["df.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2h5zcPYDb4Jf"},"source":["df.groupby(['review_rating'])['review_rating'].agg('sum')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"04owss95b7YX"},"source":["from google.colab import auth\n","auth.authenticate_user()\n","\n","# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n","project_id = 'dataimpact-rd'\n","!gcloud config set project {project_id}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"756XTPWLc1Mk"},"source":["!gsutil cp  gs://di_data_sas/Sentiment_Analysis_Related_Data/EN/weights_model_score.h5   score.h5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7x5aeVewTzxk"},"source":["!gsutil cp  gs://di_data_sas/Sentiment_Analysis_Related_Data/EN/weights_model_topics.h5   topics.h5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKxaiVPAfJCF"},"source":["!gsutil cp  gs://di_data_sas/Sentiment_Analysis_Related_Data/EN/negative_words.txt negative_words.txt "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0H8hRV97mce3"},"source":["!gsutil cp  gs://di_data_sas/Sentiment_Analysis_Related_Data/EN/positive_words.txt positive_words.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pX4BOQ2AnyTs"},"source":["### <font color='yellow'> predicting opinions Pending...</font> "]},{"cell_type":"code","metadata":{"id":"OHRVl7ASaGVR"},"source":[""],"execution_count":null,"outputs":[]}]}